{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 2\n",
    "\n",
    "## Websraping\n",
    "---\n",
    "\n",
    "Web scraping is the process of extracting data from websites automatically, typically using software or scripts, rather than by manual copying and pasting. This involves accessing the HTML code of a webpage and identifying the specific data to be extracted, such as text, images, or other media. Web scraping can be used for a variety of purposes, including data analysis, market research, and content aggregation. However, it is important to note that web scraping can raise ethical and legal concerns, particularly if it involves accessing or using proprietary or copyrighted information without permission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DevTools and site source code\n",
    "\n",
    "- Keyboard: `Ctrl + Shift + I`, except\n",
    "    - Internet Explorer and Edge: `F12`\n",
    "    - macOS: `⌘ + ⌥ + I`\n",
    "- Menu bar:\n",
    "    - Firefox: Menu   ➤ Web Developer ➤ Toggle Tools, or Tools ➤ Web Developer ➤ Toggle Tools\n",
    "    - Chrome: More tools ➤ Developer tools\n",
    "    - Safari: Develop ➤ Show Web Inspector. If you can't see the Develop menu, go to Safari ➤ Preferences ➤ Advanced, and check the Show Develop menu in menu bar checkbox.\n",
    "    -  Opera: Developer ➤ Developer tools\n",
    "- Context menu: Press-and-hold/right-click an item on a webpage (Ctrl-click on the Mac), and choose Inspect Element from the context menu that appears. (An added bonus: this method straight-away highlights the code of the element you right-clicked.)\n",
    "\n",
    "These tools are necessary for front-end development. They have use cases in Web Scraping, since we are going to use UI interactions.\n",
    "\n",
    "[Link to visit](https://pl.wikipedia.org/wiki/Wikipedia:Strona_g%C5%82%C3%B3wna)\n",
    "\n",
    "[DevTools in different browsers](https://developer.mozilla.org/en-US/docs/Learn/Common_questions/What_are_browser_developer_tools)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML\n",
    "\n",
    "All websites use HTML to work. \n",
    "Browser then interprets HTML file and creates DOM. Pure HTML websites look like skeleton [Example](http://info.cern.ch/hypertext/WWW/TheProject.html)\n",
    "By adding CSS website have styles, logic is created in a Javascript layer\n",
    "\n",
    "### Attributes\n",
    "\n",
    "Attributes for HTML tags [Global attributes](https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes).\n",
    "\n",
    "Accessibility:\n",
    "- [Highly Accessible Website - All Public Websites](https://www.gov.pl/)\n",
    "\n",
    "### HTML5 and semantic tags\n",
    "\n",
    "Semantic elements of websites [Semantics](https://developer.mozilla.org/en-US/docs/Glossary/Semantics)\n",
    "\n",
    "### Differences of websites for every country\n",
    "\n",
    "Compare these 3 websites: they are owned by a single company but there are differences on them - website is maintained differently for each region\n",
    "\n",
    "- https://www.ebay.de/\n",
    "- https://www.ebay.pl/\n",
    "- https://www.ebay.com/\n",
    "\n",
    "### Protection against web scraping\n",
    "\n",
    "- https://datadome.co/bot-management-protection/scraper-crawler-bots-how-to-protect-your-website-against-intensive-scraping\n",
    "- https://medium.com/swlh/how-to-protect-your-web-application-from-web-scraping-cc01ec7ddadd\n",
    "- Example https://www.x-kom.pl/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E-commerce website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import requests library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-3.8.5-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: requests in c:\\anaconda\\lib\\site-packages (from webdriver-manager) (2.26.0)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda\\lib\\site-packages (from webdriver-manager) (4.62.3)\n",
      "Requirement already satisfied: packaging in c:\\anaconda\\lib\\site-packages (from webdriver-manager) (21.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\anaconda\\lib\\site-packages (from webdriver-manager) (0.21.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\anaconda\\lib\\site-packages (from packaging->webdriver-manager) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests->webdriver-manager) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests->webdriver-manager) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\anaconda\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests->webdriver-manager) (1.26.7)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from tqdm->webdriver-manager) (0.4.4)\n",
      "Installing collected packages: webdriver-manager\n",
      "Successfully installed webdriver-manager-3.8.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup, element\n",
    "import time\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://webscraper.io/test-sites/e-commerce/allinone'\n",
    "res = requests.get(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just made a request. [More about HTTP methods](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods)\n",
    "To communicate with the source and retrieve resources we need to perform GET request.\n",
    "`res` is a Response object. Hover your pointer over `res` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create BeautifulSoup object from received HTML source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a popular Python library used for web scraping. It allows you to parse HTML and XML documents, and navigate through their contents in a structured way.\n",
    "\n",
    "\n",
    "#### Get page title\n",
    "\n",
    "It's the same title as we can find in our Tab name in browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Web Scraper Test Sites</title>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Web Scraper Test Sites'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.get_text()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference between `find` and `find_all` methods\n",
    "`find` returns first found item that matches given argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h2>Top items being scraped right now</h2>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2 = soup.find('h2')\n",
    "h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_all` returns list of all tags that match given argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.ResultSet'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<h1>Test Sites</h1>, <h1>E-commerce training site</h1>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1s = soup.find_all('h1')\n",
    "print(type(h1s))\n",
    "\n",
    "h1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sites\n",
      "E-commerce training site\n"
     ]
    }
   ],
   "source": [
    "# Define the type of the iterated value\n",
    "h1: element.Tag\n",
    "\n",
    "for h1 in h1s:\n",
    "    print(h1.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect all links in Sidebar navigation\n",
    "\n",
    "`find` and `find_all` methods receive an optional second argument.\n",
    "\n",
    "In this argument we pass a dictionary with defined HTML tag attributes we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the wrapper of side-menu\n",
    "sidemenu = soup.find('ul', {'id': 'side-menu'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to execute BeautifulSoup methods on a piece of taken HTML.\n",
    "\n",
    "We are working on a `side-menu` list. Let's take a peek how does it look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ul class=\"nav\" id=\"side-menu\">\n",
       "<li class=\"active\">\n",
       "<a href=\"/test-sites/e-commerce/allinone\">Home</a>\n",
       "</li>\n",
       "<li>\n",
       "<a class=\"category-link\" href=\"/test-sites/e-commerce/allinone/phones\">\n",
       "\t\t\t\t\tPhones\n",
       "\t\t\t\t\t<span class=\"fa arrow\"></span>\n",
       "</a>\n",
       "</li>\n",
       "<li>\n",
       "<a class=\"category-link\" href=\"/test-sites/e-commerce/allinone/computers\">\n",
       "\t\t\t\t\tComputers\n",
       "\t\t\t\t\t<span class=\"fa arrow\"></span>\n",
       "</a>\n",
       "</li>\n",
       "</ul>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sidemenu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every tag can have attributes like `class`, `id`, `aria-*` or `data-*` which is additional information for the browser on how it should be presented.\n",
    "\n",
    "In our case it is really useful, because we can search for HTML tags with certain attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class': ['nav'], 'id': 'side-menu'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sidemenu.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all links and save them to a variable\n",
    "anchors = sidemenu.find_all('a')\n",
    "links = []\n",
    "link: element.Tag\n",
    "for link in anchors:\n",
    "    links.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching for navigation links during web scraping is useful when building web crawlers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/test-sites/e-commerce/allinone',\n",
       " '/test-sites/e-commerce/allinone/phones',\n",
       " '/test-sites/e-commerce/allinone/computers']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">**Task 1**</span>\n",
    "\n",
    "Our client asked us to collect the products sold on [Laptop Subpage](https://webscraper.io/test-sites/e-commerce/allinone/computers/laptops) website.\n",
    "\n",
    "Gather following data and save it to a pandas DataFrame:\n",
    "- full name of the laptop,\n",
    "- description of an item\n",
    "- price in dollars, \n",
    "- number of reviews,\n",
    "- overall rating\n",
    "\n",
    "1. Find out what is the average price of laptops on this website.\n",
    "2. Sort models by rating to price ratio and find the best deal.\n",
    "3. Visualize number of laptops for each rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://webscraper.io/test-sites/e-commerce/allinone/computers/laptops'\n",
    "html = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(html._content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Asus VivoBook X4...</td>\n",
       "      <td>Asus VivoBook X441NA-GA190 Chocolate Black, 14...</td>\n",
       "      <td>$295.99</td>\n",
       "      <td>14 reviews</td>\n",
       "      <td>\\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prestigio SmartB...</td>\n",
       "      <td>Prestigio SmartBook 133S Dark Grey, 13.3\" FHD ...</td>\n",
       "      <td>$299.00</td>\n",
       "      <td>8 reviews</td>\n",
       "      <td>\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prestigio SmartB...</td>\n",
       "      <td>Prestigio SmartBook 133S Gold, 13.3\" FHD IPS, ...</td>\n",
       "      <td>$299.00</td>\n",
       "      <td>12 reviews</td>\n",
       "      <td>\\n\\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aspire E1-510</td>\n",
       "      <td>15.6\", Pentium N3520 2.16GHz, 4GB, 500GB, Linux</td>\n",
       "      <td>$306.99</td>\n",
       "      <td>2 reviews</td>\n",
       "      <td>\\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lenovo V110-15IA...</td>\n",
       "      <td>Lenovo V110-15IAP, 15.6\" HD, Celeron N3350 1.1...</td>\n",
       "      <td>$321.94</td>\n",
       "      <td>5 reviews</td>\n",
       "      <td>\\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Lenovo Legion Y7...</td>\n",
       "      <td>Lenovo Legion Y720, 15.6\" FHD IPS, Core i7-770...</td>\n",
       "      <td>$1399.00</td>\n",
       "      <td>8 reviews</td>\n",
       "      <td>\\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Asus ROG Strix G...</td>\n",
       "      <td>Asus ROG Strix GL702VM-GC146T, 17.3\" FHD, Core...</td>\n",
       "      <td>$1399.00</td>\n",
       "      <td>10 reviews</td>\n",
       "      <td>\\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Asus ROG Strix G...</td>\n",
       "      <td>Asus ROG Strix GL702ZC-GC154T, 17.3\" FHD, Ryze...</td>\n",
       "      <td>$1769.00</td>\n",
       "      <td>7 reviews</td>\n",
       "      <td>\\n\\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Asus ROG Strix G...</td>\n",
       "      <td>Asus ROG Strix GL702ZC-GC209T, 17.3\" FHD IPS, ...</td>\n",
       "      <td>$1769.00</td>\n",
       "      <td>8 reviews</td>\n",
       "      <td>\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Asus ROG Strix S...</td>\n",
       "      <td>Asus ROG Strix SCAR Edition GL503VM-ED115T, 15...</td>\n",
       "      <td>$1799.00</td>\n",
       "      <td>8 reviews</td>\n",
       "      <td>\\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name                                        Description  \\\n",
       "0    Asus VivoBook X4...  Asus VivoBook X441NA-GA190 Chocolate Black, 14...   \n",
       "1    Prestigio SmartB...  Prestigio SmartBook 133S Dark Grey, 13.3\" FHD ...   \n",
       "2    Prestigio SmartB...  Prestigio SmartBook 133S Gold, 13.3\" FHD IPS, ...   \n",
       "3          Aspire E1-510    15.6\", Pentium N3520 2.16GHz, 4GB, 500GB, Linux   \n",
       "4    Lenovo V110-15IA...  Lenovo V110-15IAP, 15.6\" HD, Celeron N3350 1.1...   \n",
       "..                   ...                                                ...   \n",
       "112  Lenovo Legion Y7...  Lenovo Legion Y720, 15.6\" FHD IPS, Core i7-770...   \n",
       "113  Asus ROG Strix G...  Asus ROG Strix GL702VM-GC146T, 17.3\" FHD, Core...   \n",
       "114  Asus ROG Strix G...  Asus ROG Strix GL702ZC-GC154T, 17.3\" FHD, Ryze...   \n",
       "115  Asus ROG Strix G...  Asus ROG Strix GL702ZC-GC209T, 17.3\" FHD IPS, ...   \n",
       "116  Asus ROG Strix S...  Asus ROG Strix SCAR Edition GL503VM-ED115T, 15...   \n",
       "\n",
       "        Price     Reviews      Rating  \n",
       "0     $295.99  14 reviews    \\n\\n\\n\\n  \n",
       "1     $299.00   8 reviews      \\n\\n\\n  \n",
       "2     $299.00  12 reviews  \\n\\n\\n\\n\\n  \n",
       "3     $306.99   2 reviews    \\n\\n\\n\\n  \n",
       "4     $321.94   5 reviews    \\n\\n\\n\\n  \n",
       "..        ...         ...         ...  \n",
       "112  $1399.00   8 reviews    \\n\\n\\n\\n  \n",
       "113  $1399.00  10 reviews    \\n\\n\\n\\n  \n",
       "114  $1769.00   7 reviews  \\n\\n\\n\\n\\n  \n",
       "115  $1769.00   8 reviews        \\n\\n  \n",
       "116  $1799.00   8 reviews    \\n\\n\\n\\n  \n",
       "\n",
       "[117 rows x 5 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code goes here\n",
    "webpage = pd.DataFrame(columns=['Name', 'Description', 'Price', 'Reviews', 'Rating'])\n",
    "\n",
    "price = soup.find_all('h4', {'class': 'pull-right price'})\n",
    "description = soup.find_all('p', {'class': 'description'})\n",
    "name = soup.find_all('a', {'class': 'title'})\n",
    "reviews = soup.find_all('p', {'class': 'pull-right'})\n",
    "\n",
    "rating = soup.find_all('p', {'data-rating': True})\n",
    "\n",
    "webpage['Name'] = [i.get_text() for i in name]\n",
    "webpage['Description'] = [i.get_text() for i in description]\n",
    "webpage['Price'] = [i.get_text() for i in price]\n",
    "webpage['Reviews'] = [i.get_text() for i in reviews]\n",
    "webpage['Rating'] = [i.get_text() for i in rating]\n",
    "\n",
    "\n",
    "\n",
    "webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = soup.find_all('p', {'data-rating': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract <p data-rating=\"3\">\n",
    "rating\n",
    "\n",
    "#extract 3\n",
    "rating[0].get('data-rating')\n",
    "\n",
    "#extract 3\n",
    "rating[0].get_text()    \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium - Human interactions on the browser\n",
    "\n",
    "Used for dynamic websites, websites behind login forms, websites with unpredictable urls.\n",
    "\n",
    "In this chapter we want to automate interaction on the browser. \n",
    "Sometimes certain pages cannot be easily accessed - i.e they stay behind login forms, \n",
    "exist under unpredictable url or have to be accessed via click of the button.\n",
    "\n",
    "In this case we need to use Selenium library.\n",
    "\n",
    "Let's download **webdriver** of your current browser.\n",
    "\n",
    "[Installation guide for selenium](https://selenium-python.readthedocs.io/installation.html#)\n",
    "\n",
    "[Drivers](https://selenium-python.readthedocs.io/installation.html#drivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Downloading: 100%|██████████| 6.78M/6.78M [00:00<00:00, 10.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get('https://pythonscraping.com/linkedin/cookies/login.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "username_input = driver.find_element(by='name', value='username')\n",
    "username_input.send_keys('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "username_input = driver.find_element('name', 'password')\n",
    "username_input.send_keys('password')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "button = driver.find_element(By.XPATH, \"//input[@type='submit']\")\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the link that has content `Check out your profile!`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[text() = 'Check out your profile!']\"}\n  (Session info: chrome=110.0.5481.178)\nStacktrace:\nBacktrace:\n\t(No symbol) [0x00DE37D3]\n\t(No symbol) [0x00D78B81]\n\t(No symbol) [0x00C7B36D]\n\t(No symbol) [0x00CAD382]\n\t(No symbol) [0x00CAD4BB]\n\t(No symbol) [0x00CE3302]\n\t(No symbol) [0x00CCB464]\n\t(No symbol) [0x00CE1215]\n\t(No symbol) [0x00CCB216]\n\t(No symbol) [0x00CA0D97]\n\t(No symbol) [0x00CA253D]\n\tGetHandleVerifier [0x0105ABF2+2510930]\n\tGetHandleVerifier [0x01088EC1+2700065]\n\tGetHandleVerifier [0x0108C86C+2714828]\n\tGetHandleVerifier [0x00E93480+645344]\n\t(No symbol) [0x00D80FD2]\n\t(No symbol) [0x00D86C68]\n\t(No symbol) [0x00D86D4B]\n\t(No symbol) [0x00D90D6B]\n\tBaseThreadInitThunk [0x76AE00F9+25]\n\tRtlGetAppContainerNamedObjectPath [0x77287BBE+286]\n\tRtlGetAppContainerNamedObjectPath [0x77287B8E+238]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\JAKUBZ~1\\AppData\\Local\\Temp/ipykernel_9064/880554199.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"//*[text() = 'Check out your profile!']\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    828\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'[name=\"{value}\"]'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 830\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFIND_ELEMENT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"using\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"value\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"value\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mWebElement\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    438\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"value\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unwrap_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"alert\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[text() = 'Check out your profile!']\"}\n  (Session info: chrome=110.0.5481.178)\nStacktrace:\nBacktrace:\n\t(No symbol) [0x00DE37D3]\n\t(No symbol) [0x00D78B81]\n\t(No symbol) [0x00C7B36D]\n\t(No symbol) [0x00CAD382]\n\t(No symbol) [0x00CAD4BB]\n\t(No symbol) [0x00CE3302]\n\t(No symbol) [0x00CCB464]\n\t(No symbol) [0x00CE1215]\n\t(No symbol) [0x00CCB216]\n\t(No symbol) [0x00CA0D97]\n\t(No symbol) [0x00CA253D]\n\tGetHandleVerifier [0x0105ABF2+2510930]\n\tGetHandleVerifier [0x01088EC1+2700065]\n\tGetHandleVerifier [0x0108C86C+2714828]\n\tGetHandleVerifier [0x00E93480+645344]\n\t(No symbol) [0x00D80FD2]\n\t(No symbol) [0x00D86C68]\n\t(No symbol) [0x00D86D4B]\n\t(No symbol) [0x00D90D6B]\n\tBaseThreadInitThunk [0x76AE00F9+25]\n\tRtlGetAppContainerNamedObjectPath [0x77287BBE+286]\n\tRtlGetAppContainerNamedObjectPath [0x77287B8E+238]\n"
     ]
    }
   ],
   "source": [
    "link = driver.find_element(By.XPATH, \"//*[text() = 'Check out your profile!']\")\n",
    "link.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get('https://www.booking.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Cookies banner\n",
    "cookie_banner = driver.find_element(By.ID,'onetrust-accept-btn-handler')\n",
    "cookie_banner.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_input = driver.find_element(By.XPATH,\"//input[@name='ss']\")\n",
    "search_input.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_input.send_keys('Warszawa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datepicker = driver.find_element(By.XPATH,'//div[@data-testid=\"searchbox-dates-container\"]')\n",
    "datepicker.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_button = driver.find_element(By.XPATH,\"//span[@aria-label='20 March 2023']\")\n",
    "start_date_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_button = driver.find_element(By.XPATH,\"//span[@aria-label='23 March 2023']\")\n",
    "start_date_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button = driver.find_element(By.XPATH,\"//button[@type='submit']\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_4_star = driver.find_element(By.XPATH,\"//div[text()='4 stars']\")\n",
    "\n",
    "ActionChains(driver).move_to_element(filter_4_star).perform()\n",
    "\n",
    "filter_4_star.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data\n",
    "soup = BeautifulSoup(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "properties = soup.find_all('div', {'data-testid': 'property-card'})\n",
    "prop: element.Tag\n",
    "hotels = list()\n",
    "for prop in properties:\n",
    "  hotels.append({\n",
    "    'title':  prop.find('div', {'data-testid': 'title'}).text,\n",
    "    'address': prop.find('span', {'data-testid': 'address'}).text,\n",
    "    'distance': prop.find('span', {'data-testid': 'distance'}).text,\n",
    "    'price': prop.find('span', {'data-testid': 'price-and-discounted-price'}).text,\n",
    "    'review': prop.find('div', {'data-testid': 'review-score'}).find('div').text\n",
    "  })\n",
    "  \n",
    "pd.DataFrame(hotels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put that into a single piece of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(source, city: str):\n",
    "    soup = BeautifulSoup(source)\n",
    "    properties = soup.find_all('div', {'data-testid': 'property-card'})\n",
    "    hotels = list()\n",
    "    for prop in properties:\n",
    "        try:\n",
    "            hotels.append({\n",
    "                'city': city,\n",
    "                'title':  prop.find('div', {'data-testid': 'title'}).text,\n",
    "                'address': prop.find('span', {'data-testid': 'address'}).text,\n",
    "                'distance': prop.find('span', {'data-testid': 'distance'}).text,\n",
    "                'price': prop.find('span', {'data-testid': 'price-and-discounted-price'}).text,\n",
    "                'review': prop.find('div', {'data-testid': 'review-score'}).find('div').text\n",
    "            })\n",
    "        except:\n",
    "            pass\n",
    "    return hotels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_booking(city: str):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get('https://www.booking.com/')\n",
    "    try:\n",
    "        time.sleep(3)  \n",
    "        # Close Cookies banner - it blocks some visibility\n",
    "        cookie_banner = driver.find_element(By.ID,'onetrust-accept-btn-handler')\n",
    "        cookie_banner.click()\n",
    "        \n",
    "        search_input = driver.find_element(By.XPATH,\"//input[@name='ss']\")\n",
    "        search_input.click()\n",
    "        time.sleep(1)  \n",
    "        search_input.send_keys(city)\n",
    "        time.sleep(1)          \n",
    "        # Select date\n",
    "        datepicker = driver.find_element(By.XPATH,'//div[@data-testid=\"searchbox-dates-container\"]')\n",
    "        datepicker.click()\n",
    "        time.sleep(1)  \n",
    "        start_date_button = driver.find_element(By.XPATH,\"//span[@aria-label='20 March 2023']\")\n",
    "        start_date_button.click()\n",
    "        time.sleep(1)  \n",
    "        start_date_button = driver.find_element(By.XPATH,\"//span[@aria-label='23 March 2023']\")\n",
    "        start_date_button.click()\n",
    "        time.sleep(1)  \n",
    "        \n",
    "        # Press search\n",
    "        search_button = driver.find_element(By.XPATH,\"//button[@type='submit']\")\n",
    "        search_button.click()\n",
    "        \n",
    "        # Filter only 4 star hotels\n",
    "        filter_4_star = driver.find_element(By.XPATH,\"//div[text()='4 stars']\")\n",
    "\n",
    "        ActionChains(driver).move_to_element(filter_4_star).perform()\n",
    "\n",
    "        filter_4_star.click()\n",
    "\n",
    "        # Wait until the results are visible\n",
    "        WebDriverWait(driver, 10).until(EC.invisibility_of_element_located((By.XPATH, \"//div[@data-testid='overlay-wrapper']\")))\n",
    "        # Extract data and save it in the dataframe\n",
    "        \n",
    "        data = collect_data(driver.page_source, city)\n",
    "        driver.quit()\n",
    "        return data\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_list = ['Warszawa']\n",
    "hotels = pd.DataFrame()\n",
    "\n",
    "for city in cities_list:\n",
    "    hotels = hotels.append(scrape_booking(city), ignore_index=True)\n",
    "    \n",
    "hotels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">**TASK & HOMEWORK**</span>\n",
    "\n",
    "Automate the process of getting details for restaurants from [TripAdvisor](https://www.tripadvisor.com/) website in Warsaw. \n",
    "\n",
    "Get data for first 3 result pages.\n",
    "\n",
    "Gather following data and save it to a pandas DataFrame:\n",
    "- name of the restaurant,\n",
    "- rating,\n",
    "- number of reviews,\n",
    "- link to visit details page,\n",
    "- address,\n",
    "- phone number (if existst),\n",
    "- website url (if exists),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code goes here\n",
    "\n",
    "def scrape_booking(city: str):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get('https://www.tripadvisor.com/')\n",
    "    try:\n",
    "        time.sleep(3)  \n",
    "        # Close Cookies banner - it blocks some visibility\n",
    "        cookie_banner = driver.find_element(By.ID,'onetrust-accept-btn-handler')\n",
    "        cookie_banner.click()\n",
    "        \n",
    "        search_input = driver.find_element(By.XPATH,\"//*[@id=\"lithium-root\"]/main/div[1]/div[1]/div/div/div[4]/a]\")\n",
    "        search_input.click()\n",
    "        time.sleep(1)  \n",
    "        search_input.send_keys(city)\n",
    "        time.sleep(1)          \n",
    "        # Select date\n",
    "        datepicker = driver.find_element(By.XPATH,'//div[@data-testid=\"searchbox-dates-container\"]')\n",
    "        datepicker.click()\n",
    "        time.sleep(1)  \n",
    "        start_date_button = driver.find_element(By.XPATH,\"//span[@aria-label='20 March 2023']\")\n",
    "        start_date_button.click()\n",
    "        time.sleep(1)  \n",
    "        start_date_button = driver.find_element(By.XPATH,\"//span[@aria-label='23 March 2023']\")\n",
    "        start_date_button.click()\n",
    "        time.sleep(1)  \n",
    "        \n",
    "        # Press search\n",
    "        search_button = driver.find_element(By.XPATH,\"//button[@type='submit']\")\n",
    "        search_button.click()\n",
    "        \n",
    "        # Filter only 4 star hotels\n",
    "        filter_4_star = driver.find_element(By.XPATH,\"//div[text()='4 stars']\")\n",
    "\n",
    "        ActionChains(driver).move_to_element(filter_4_star).perform()\n",
    "\n",
    "        filter_4_star.click()\n",
    "\n",
    "        # Wait until the results are visible\n",
    "        WebDriverWait(driver, 10).until(EC.invisibility_of_element_located((By.XPATH, \"//div[@data-testid='overlay-wrapper']\")))\n",
    "        # Extract data and save it in the dataframe\n",
    "        \n",
    "        data = collect_data(driver.page_source, city)\n",
    "        driver.quit()\n",
    "        return data\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "---\n",
    "### Web Scraping\n",
    "- [Web Scraping with Python](https://www.scrapethissite.com/pages/)\n",
    "\n",
    "### Beautiful Soup\n",
    "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Beautiful Soup Tutorial](https://www.dataquest.io/blog/web-scraping-tutorial-python/)\n",
    "- [Beautiful Soup Tutorial Youtube](https://www.youtube.com/watch?v=ng2o98k983k)\n",
    "\n",
    "### Selenium\n",
    "- [Selenium Documentation](https://selenium-python.readthedocs.io/)\n",
    "- [Complex Selenium Tutorial in Java](https://www.guru99.com/selenium-tutorial.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f5583cf1d9466b5c27e75c89cc6b383bed5736d6b16c51c8074d8690011a952"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

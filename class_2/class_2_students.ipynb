{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 2\n",
    "\n",
    "## Websraping\n",
    "---\n",
    "\n",
    "Web scraping is the process of extracting data from websites automatically, typically using software or scripts, rather than by manual copying and pasting. This involves accessing the HTML code of a webpage and identifying the specific data to be extracted, such as text, images, or other media. Web scraping can be used for a variety of purposes, including data analysis, market research, and content aggregation. However, it is important to note that web scraping can raise ethical and legal concerns, particularly if it involves accessing or using proprietary or copyrighted information without permission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DevTools and site source code\n",
    "\n",
    "- Keyboard: `Ctrl + Shift + I`, except\n",
    "    - Internet Explorer and Edge: `F12`\n",
    "    - macOS: `⌘ + ⌥ + I`\n",
    "- Menu bar:\n",
    "    - Firefox: Menu   ➤ Web Developer ➤ Toggle Tools, or Tools ➤ Web Developer ➤ Toggle Tools\n",
    "    - Chrome: More tools ➤ Developer tools\n",
    "    - Safari: Develop ➤ Show Web Inspector. If you can't see the Develop menu, go to Safari ➤ Preferences ➤ Advanced, and check the Show Develop menu in menu bar checkbox.\n",
    "    -  Opera: Developer ➤ Developer tools\n",
    "- Context menu: Press-and-hold/right-click an item on a webpage (Ctrl-click on the Mac), and choose Inspect Element from the context menu that appears. (An added bonus: this method straight-away highlights the code of the element you right-clicked.)\n",
    "\n",
    "These tools are necessary for front-end development. They have use cases in Web Scraping, since we are going to use UI interactions.\n",
    "\n",
    "[Link to visit](https://pl.wikipedia.org/wiki/Wikipedia:Strona_g%C5%82%C3%B3wna)\n",
    "\n",
    "[DevTools in different browsers](https://developer.mozilla.org/en-US/docs/Learn/Common_questions/What_are_browser_developer_tools)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML\n",
    "\n",
    "All websites use HTML to work. \n",
    "Browser then interprets HTML file and creates DOM. Pure HTML websites look like skeleton [Example](http://info.cern.ch/hypertext/WWW/TheProject.html)\n",
    "By adding CSS website have styles, logic is created in a Javascript layer\n",
    "\n",
    "### Attributes\n",
    "\n",
    "Attributes for HTML tags [Global attributes](https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes).\n",
    "\n",
    "Accessibility:\n",
    "- [Highly Accessible Website - All Public Websites](https://www.gov.pl/)\n",
    "\n",
    "### HTML5 and semantic tags\n",
    "\n",
    "Semantic elements of websites [Semantics](https://developer.mozilla.org/en-US/docs/Glossary/Semantics)\n",
    "\n",
    "### Differences of websites for every country\n",
    "\n",
    "Compare these 3 websites: they are owned by a single company but there are differences on them - website is maintained differently for each region\n",
    "\n",
    "- https://www.ebay.de/\n",
    "- https://www.ebay.pl/\n",
    "- https://www.ebay.com/\n",
    "\n",
    "### Protection against web scraping\n",
    "\n",
    "- https://datadome.co/bot-management-protection/scraper-crawler-bots-how-to-protect-your-website-against-intensive-scraping\n",
    "- https://medium.com/swlh/how-to-protect-your-web-application-from-web-scraping-cc01ec7ddadd\n",
    "- Example https://www.x-kom.pl/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E-commerce website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import requests library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup, element\n",
    "import time\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://webscraper.io/test-sites/e-commerce/allinone'\n",
    "res = requests.get(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just made a request. [More about HTTP methods](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods)\n",
    "To communicate with the source and retrieve resources we need to perform GET request.\n",
    "`res` is a Response object. Hover your pointer over `res` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create BeautifulSoup object from received HTML source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a popular Python library used for web scraping. It allows you to parse HTML and XML documents, and navigate through their contents in a structured way.\n",
    "\n",
    "\n",
    "#### Get page title\n",
    "\n",
    "It's the same title as we can find in our Tab name in browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Web Scraper Test Sites</title>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Web Scraper Test Sites'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.get_text()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference between `find` and `find_all` methods\n",
    "`find` returns first found item that matches given argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h2>Top items being scraped right now</h2>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2 = soup.find('h2')\n",
    "h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_all` returns list of all tags that match given argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.ResultSet'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<h1>Test Sites</h1>, <h1>E-commerce training site</h1>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1s = soup.find_all('h1')\n",
    "print(type(h1s))\n",
    "\n",
    "h1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sites\n",
      "E-commerce training site\n"
     ]
    }
   ],
   "source": [
    "# Define the type of the iterated value\n",
    "h1: element.Tag\n",
    "\n",
    "for h1 in h1s:\n",
    "    print(h1.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect all links in Sidebar navigation\n",
    "\n",
    "`find` and `find_all` methods receive an optional second argument.\n",
    "\n",
    "In this argument we pass a dictionary with defined HTML tag attributes we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the wrapper of side-menu\n",
    "sidemenu = soup.find('ul', {'id': 'side-menu'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to execute BeautifulSoup methods on a piece of taken HTML.\n",
    "\n",
    "We are working on a `side-menu` list. Let's take a peek how does it look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ul class=\"nav\" id=\"side-menu\">\n",
       "<li class=\"active\">\n",
       "<a href=\"/test-sites/e-commerce/allinone\">Home</a>\n",
       "</li>\n",
       "<li>\n",
       "<a class=\"category-link\" href=\"/test-sites/e-commerce/allinone/phones\">\n",
       "\t\t\t\t\tPhones\n",
       "\t\t\t\t\t<span class=\"fa arrow\"></span>\n",
       "</a>\n",
       "</li>\n",
       "<li>\n",
       "<a class=\"category-link\" href=\"/test-sites/e-commerce/allinone/computers\">\n",
       "\t\t\t\t\tComputers\n",
       "\t\t\t\t\t<span class=\"fa arrow\"></span>\n",
       "</a>\n",
       "</li>\n",
       "</ul>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sidemenu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every tag can have attributes like `class`, `id`, `aria-*` or `data-*` which is additional information for the browser on how it should be presented.\n",
    "\n",
    "In our case it is really useful, because we can search for HTML tags with certain attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class': ['nav'], 'id': 'side-menu'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sidemenu.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all links and save them to a variable\n",
    "anchors = sidemenu.find_all('a')\n",
    "links = []\n",
    "link: element.Tag\n",
    "for link in anchors:\n",
    "    links.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching for navigation links during web scraping is useful when building web crawlers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/test-sites/e-commerce/allinone',\n",
       " '/test-sites/e-commerce/allinone/phones',\n",
       " '/test-sites/e-commerce/allinone/computers']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">**Task 1**</span>\n",
    "\n",
    "Our client asked us to collect the products sold on [Laptop Subpage](https://webscraper.io/test-sites/e-commerce/allinone/computers/laptops) website.\n",
    "\n",
    "Gather following data and save it to a pandas DataFrame:\n",
    "- full name of the laptop,\n",
    "- description of an item\n",
    "- price in dollars, \n",
    "- number of reviews,\n",
    "- overall rating\n",
    "\n",
    "1. Find out what is the average price of laptops on this website.\n",
    "2. Sort models by rating to price ratio and find the best deal.\n",
    "3. Visualize number of laptops for each rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://webscraper.io/test-sites/e-commerce/allinone/computers/laptops'\n",
    "html = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(html._content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Asus VivoBook X4...</td>\n",
       "      <td>Asus VivoBook X441NA-GA190 Chocolate Black, 14...</td>\n",
       "      <td>$295.99</td>\n",
       "      <td>14 reviews</td>\n",
       "      <td>\\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prestigio SmartB...</td>\n",
       "      <td>Prestigio SmartBook 133S Dark Grey, 13.3\" FHD ...</td>\n",
       "      <td>$299.00</td>\n",
       "      <td>8 reviews</td>\n",
       "      <td>\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prestigio SmartB...</td>\n",
       "      <td>Prestigio SmartBook 133S Gold, 13.3\" FHD IPS, ...</td>\n",
       "      <td>$299.00</td>\n",
       "      <td>12 reviews</td>\n",
       "      <td>\\n\\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aspire E1-510</td>\n",
       "      <td>15.6\", Pentium N3520 2.16GHz, 4GB, 500GB, Linux</td>\n",
       "      <td>$306.99</td>\n",
       "      <td>2 reviews</td>\n",
       "      <td>\\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lenovo V110-15IA...</td>\n",
       "      <td>Lenovo V110-15IAP, 15.6\" HD, Celeron N3350 1.1...</td>\n",
       "      <td>$321.94</td>\n",
       "      <td>5 reviews</td>\n",
       "      <td>\\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Lenovo Legion Y7...</td>\n",
       "      <td>Lenovo Legion Y720, 15.6\" FHD IPS, Core i7-770...</td>\n",
       "      <td>$1399.00</td>\n",
       "      <td>8 reviews</td>\n",
       "      <td>\\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Asus ROG Strix G...</td>\n",
       "      <td>Asus ROG Strix GL702VM-GC146T, 17.3\" FHD, Core...</td>\n",
       "      <td>$1399.00</td>\n",
       "      <td>10 reviews</td>\n",
       "      <td>\\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Asus ROG Strix G...</td>\n",
       "      <td>Asus ROG Strix GL702ZC-GC154T, 17.3\" FHD, Ryze...</td>\n",
       "      <td>$1769.00</td>\n",
       "      <td>7 reviews</td>\n",
       "      <td>\\n\\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Asus ROG Strix G...</td>\n",
       "      <td>Asus ROG Strix GL702ZC-GC209T, 17.3\" FHD IPS, ...</td>\n",
       "      <td>$1769.00</td>\n",
       "      <td>8 reviews</td>\n",
       "      <td>\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Asus ROG Strix S...</td>\n",
       "      <td>Asus ROG Strix SCAR Edition GL503VM-ED115T, 15...</td>\n",
       "      <td>$1799.00</td>\n",
       "      <td>8 reviews</td>\n",
       "      <td>\\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name                                        Description  \\\n",
       "0    Asus VivoBook X4...  Asus VivoBook X441NA-GA190 Chocolate Black, 14...   \n",
       "1    Prestigio SmartB...  Prestigio SmartBook 133S Dark Grey, 13.3\" FHD ...   \n",
       "2    Prestigio SmartB...  Prestigio SmartBook 133S Gold, 13.3\" FHD IPS, ...   \n",
       "3          Aspire E1-510    15.6\", Pentium N3520 2.16GHz, 4GB, 500GB, Linux   \n",
       "4    Lenovo V110-15IA...  Lenovo V110-15IAP, 15.6\" HD, Celeron N3350 1.1...   \n",
       "..                   ...                                                ...   \n",
       "112  Lenovo Legion Y7...  Lenovo Legion Y720, 15.6\" FHD IPS, Core i7-770...   \n",
       "113  Asus ROG Strix G...  Asus ROG Strix GL702VM-GC146T, 17.3\" FHD, Core...   \n",
       "114  Asus ROG Strix G...  Asus ROG Strix GL702ZC-GC154T, 17.3\" FHD, Ryze...   \n",
       "115  Asus ROG Strix G...  Asus ROG Strix GL702ZC-GC209T, 17.3\" FHD IPS, ...   \n",
       "116  Asus ROG Strix S...  Asus ROG Strix SCAR Edition GL503VM-ED115T, 15...   \n",
       "\n",
       "        Price     Reviews      Rating  \n",
       "0     $295.99  14 reviews    \\n\\n\\n\\n  \n",
       "1     $299.00   8 reviews      \\n\\n\\n  \n",
       "2     $299.00  12 reviews  \\n\\n\\n\\n\\n  \n",
       "3     $306.99   2 reviews    \\n\\n\\n\\n  \n",
       "4     $321.94   5 reviews    \\n\\n\\n\\n  \n",
       "..        ...         ...         ...  \n",
       "112  $1399.00   8 reviews    \\n\\n\\n\\n  \n",
       "113  $1399.00  10 reviews    \\n\\n\\n\\n  \n",
       "114  $1769.00   7 reviews  \\n\\n\\n\\n\\n  \n",
       "115  $1769.00   8 reviews        \\n\\n  \n",
       "116  $1799.00   8 reviews    \\n\\n\\n\\n  \n",
       "\n",
       "[117 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code goes here\n",
    "webpage = pd.DataFrame(columns=['Name', 'Description', 'Price', 'Reviews', 'Rating'])\n",
    "\n",
    "price = soup.find_all('h4', {'class': 'pull-right price'})\n",
    "description = soup.find_all('p', {'class': 'description'})\n",
    "name = soup.find_all('a', {'class': 'title'})\n",
    "reviews = soup.find_all('p', {'class': 'pull-right'})\n",
    "\n",
    "rating = soup.find_all('p', {'data-rating': True})\n",
    "\n",
    "webpage['Name'] = [i.get_text() for i in name]\n",
    "webpage['Description'] = [i.get_text() for i in description]\n",
    "webpage['Price'] = [i.get_text() for i in price]\n",
    "webpage['Reviews'] = [i.get_text() for i in reviews]\n",
    "webpage['Rating'] = [i.get_text() for i in rating]\n",
    "\n",
    "\n",
    "\n",
    "webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = soup.find_all('p', {'data-rating': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract <p data-rating=\"3\">\n",
    "rating\n",
    "\n",
    "#extract 3\n",
    "rating[0].get('data-rating')\n",
    "\n",
    "#extract 3\n",
    "rating[0].get_text()    \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium - Human interactions on the browser\n",
    "\n",
    "Used for dynamic websites, websites behind login forms, websites with unpredictable urls.\n",
    "\n",
    "In this chapter we want to automate interaction on the browser. \n",
    "Sometimes certain pages cannot be easily accessed - i.e they stay behind login forms, \n",
    "exist under unpredictable url or have to be accessed via click of the button.\n",
    "\n",
    "In this case we need to use Selenium library.\n",
    "\n",
    "Let's download **webdriver** of your current browser.\n",
    "\n",
    "[Installation guide for selenium](https://selenium-python.readthedocs.io/installation.html#)\n",
    "\n",
    "[Drivers](https://selenium-python.readthedocs.io/installation.html#drivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get('https://pythonscraping.com/linkedin/cookies/login.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "username_input = driver.find_element(by='name', value='username')\n",
    "username_input.send_keys('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "username_input = driver.find_element('name', 'password')\n",
    "username_input.send_keys('password')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "button = driver.find_element(By.XPATH, \"//input[@type='submit']\")\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the link that has content `Check out your profile!`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = driver.find_element(By.XPATH, \"//*[text() = 'Check out your profile!']\")\n",
    "link.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get('https://www.booking.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[id=\"onetrust-accept-btn-handler\"]\"}\n  (Session info: chrome=110.0.5481.178)\nStacktrace:\nBacktrace:\n\t(No symbol) [0x00CB37D3]\n\t(No symbol) [0x00C48B81]\n\t(No symbol) [0x00B4B36D]\n\t(No symbol) [0x00B7D382]\n\t(No symbol) [0x00B7D4BB]\n\t(No symbol) [0x00BB3302]\n\t(No symbol) [0x00B9B464]\n\t(No symbol) [0x00BB1215]\n\t(No symbol) [0x00B9B216]\n\t(No symbol) [0x00B70D97]\n\t(No symbol) [0x00B7253D]\n\tGetHandleVerifier [0x00F2ABF2+2510930]\n\tGetHandleVerifier [0x00F58EC1+2700065]\n\tGetHandleVerifier [0x00F5C86C+2714828]\n\tGetHandleVerifier [0x00D63480+645344]\n\t(No symbol) [0x00C50FD2]\n\t(No symbol) [0x00C56C68]\n\t(No symbol) [0x00C56D4B]\n\t(No symbol) [0x00C60D6B]\n\tBaseThreadInitThunk [0x768500F9+25]\n\tRtlGetAppContainerNamedObjectPath [0x76F87BBE+286]\n\tRtlGetAppContainerNamedObjectPath [0x76F87B8E+238]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Close Cookies banner\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m cookie_banner \u001b[39m=\u001b[39m driver\u001b[39m.\u001b[39;49mfind_element(By\u001b[39m.\u001b[39;49mID,\u001b[39m'\u001b[39;49m\u001b[39monetrust-accept-btn-handler\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m cookie_banner\u001b[39m.\u001b[39mclick()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:830\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    827\u001b[0m     by \u001b[39m=\u001b[39m By\u001b[39m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    828\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[name=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 830\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mFIND_ELEMENT, {\u001b[39m\"\u001b[39;49m\u001b[39musing\u001b[39;49m\u001b[39m\"\u001b[39;49m: by, \u001b[39m\"\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m\"\u001b[39;49m: value})[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:440\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    438\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_executor\u001b[39m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[1;32m--> 440\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_handler\u001b[39m.\u001b[39;49mcheck_response(response)\n\u001b[0;32m    441\u001b[0m     response[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unwrap_value(response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    442\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:245\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    243\u001b[0m         alert_text \u001b[39m=\u001b[39m value[\u001b[39m\"\u001b[39m\u001b[39malert\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    244\u001b[0m     \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[39m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 245\u001b[0m \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[id=\"onetrust-accept-btn-handler\"]\"}\n  (Session info: chrome=110.0.5481.178)\nStacktrace:\nBacktrace:\n\t(No symbol) [0x00CB37D3]\n\t(No symbol) [0x00C48B81]\n\t(No symbol) [0x00B4B36D]\n\t(No symbol) [0x00B7D382]\n\t(No symbol) [0x00B7D4BB]\n\t(No symbol) [0x00BB3302]\n\t(No symbol) [0x00B9B464]\n\t(No symbol) [0x00BB1215]\n\t(No symbol) [0x00B9B216]\n\t(No symbol) [0x00B70D97]\n\t(No symbol) [0x00B7253D]\n\tGetHandleVerifier [0x00F2ABF2+2510930]\n\tGetHandleVerifier [0x00F58EC1+2700065]\n\tGetHandleVerifier [0x00F5C86C+2714828]\n\tGetHandleVerifier [0x00D63480+645344]\n\t(No symbol) [0x00C50FD2]\n\t(No symbol) [0x00C56C68]\n\t(No symbol) [0x00C56D4B]\n\t(No symbol) [0x00C60D6B]\n\tBaseThreadInitThunk [0x768500F9+25]\n\tRtlGetAppContainerNamedObjectPath [0x76F87BBE+286]\n\tRtlGetAppContainerNamedObjectPath [0x76F87B8E+238]\n"
     ]
    }
   ],
   "source": [
    "# Close Cookies banner\n",
    "cookie_banner = driver.find_element(By.ID,'onetrust-accept-btn-handler')\n",
    "cookie_banner.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_input = driver.find_element(By.XPATH,\"//input[@name='ss']\")\n",
    "search_input.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_input.send_keys('Warszawa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datepicker = driver.find_element(By.XPATH,'//div[@data-testid=\"searchbox-dates-container\"]')\n",
    "datepicker.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_button = driver.find_element(By.XPATH,\"//span[@aria-label='20 March 2023']\")\n",
    "start_date_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_button = driver.find_element(By.XPATH,\"//span[@aria-label='23 March 2023']\")\n",
    "start_date_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button = driver.find_element(By.XPATH,\"//button[@type='submit']\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_4_star = driver.find_element(By.XPATH,\"//div[text()='4 stars']\")\n",
    "\n",
    "ActionChains(driver).move_to_element(filter_4_star).perform()\n",
    "\n",
    "filter_4_star.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data\n",
    "soup = BeautifulSoup(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "properties = soup.find_all('div', {'data-testid': 'property-card'})\n",
    "prop: element.Tag\n",
    "hotels = list()\n",
    "for prop in properties:\n",
    "  hotels.append({\n",
    "    'title':  prop.find('div', {'data-testid': 'title'}).text,\n",
    "    'address': prop.find('span', {'data-testid': 'address'}).text,\n",
    "    'distance': prop.find('span', {'data-testid': 'distance'}).text,\n",
    "    'price': prop.find('span', {'data-testid': 'price-and-discounted-price'}).text,\n",
    "    'review': prop.find('div', {'data-testid': 'review-score'}).find('div').text\n",
    "  })\n",
    "  \n",
    "pd.DataFrame(hotels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put that into a single piece of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(source, city: str):\n",
    "    soup = BeautifulSoup(source)\n",
    "    properties = soup.find_all('div', {'data-testid': 'property-card'})\n",
    "    hotels = list()\n",
    "    for prop in properties:\n",
    "        try:\n",
    "            hotels.append({\n",
    "                'city': city,\n",
    "                'title':  prop.find('div', {'data-testid': 'title'}).text,\n",
    "                'address': prop.find('span', {'data-testid': 'address'}).text,\n",
    "                'distance': prop.find('span', {'data-testid': 'distance'}).text,\n",
    "                'price': prop.find('span', {'data-testid': 'price-and-discounted-price'}).text,\n",
    "                'review': prop.find('div', {'data-testid': 'review-score'}).find('div').text\n",
    "            })\n",
    "        except:\n",
    "            pass\n",
    "    return hotels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_booking(city: str):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get('https://www.booking.com/')\n",
    "    try:\n",
    "        time.sleep(3)  \n",
    "        # Close Cookies banner - it blocks some visibility\n",
    "        cookie_banner = driver.find_element(By.ID,'onetrust-accept-btn-handler')\n",
    "        cookie_banner.click()\n",
    "        \n",
    "        search_input = driver.find_element(By.XPATH,\"//input[@name='ss']\")\n",
    "        search_input.click()\n",
    "        time.sleep(1)  \n",
    "        search_input.send_keys(city)\n",
    "        time.sleep(1)          \n",
    "        # Select date\n",
    "        datepicker = driver.find_element(By.XPATH,'//div[@data-testid=\"searchbox-dates-container\"]')\n",
    "        datepicker.click()\n",
    "        time.sleep(1)  \n",
    "        start_date_button = driver.find_element(By.XPATH,\"//span[@aria-label='20 March 2023']\")\n",
    "        start_date_button.click()\n",
    "        time.sleep(1)  \n",
    "        start_date_button = driver.find_element(By.XPATH,\"//span[@aria-label='23 March 2023']\")\n",
    "        start_date_button.click()\n",
    "        time.sleep(1)  \n",
    "        \n",
    "        # Press search\n",
    "        search_button = driver.find_element(By.XPATH,\"//button[@type='submit']\")\n",
    "        search_button.click()\n",
    "        \n",
    "        # Filter only 4 star hotels\n",
    "        filter_4_star = driver.find_element(By.XPATH,\"//div[text()='4 stars']\")\n",
    "\n",
    "        ActionChains(driver).move_to_element(filter_4_star).perform()\n",
    "\n",
    "        filter_4_star.click()\n",
    "\n",
    "        # Wait until the results are visible\n",
    "        WebDriverWait(driver, 10).until(EC.invisibility_of_element_located((By.XPATH, \"//div[@data-testid='overlay-wrapper']\")))\n",
    "        # Extract data and save it in the dataframe\n",
    "        \n",
    "        data = collect_data(driver.page_source, city)\n",
    "        driver.quit()\n",
    "        return data\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Downloading: 100%|██████████| 6.78M/6.78M [00:00<00:00, 58.0MB/s]\n",
      "C:\\Users\\zapas\\AppData\\Local\\Temp\\ipykernel_13912\\300646056.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  hotels = hotels.append(scrape_booking(city), ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities_list = ['Warszawa']\n",
    "hotels = pd.DataFrame()\n",
    "\n",
    "for city in cities_list:\n",
    "    hotels = hotels.append(scrape_booking(city), ignore_index=True)\n",
    "    \n",
    "hotels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">**TASK & HOMEWORK**</span>\n",
    "\n",
    "Automate the process of getting details for restaurants from [TripAdvisor](https://www.tripadvisor.com/) website in Warsaw. \n",
    "\n",
    "Get data for first 3 result pages.\n",
    "\n",
    "Gather following data and save it to a pandas DataFrame:\n",
    "- name of the restaurant,\n",
    "- rating,\n",
    "- number of reviews,\n",
    "- link to visit details page,\n",
    "- address,\n",
    "- phone number (if existst),\n",
    "- website url (if exists),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_tripadvisor_restaurants(city_name, num_restaurants):\n",
    "    # Set up web driver and navigate to Tripadvisor    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get('https://www.tripadvisor.com/')\n",
    "    time.sleep(3)\n",
    "    # Click cookie accept button    \n",
    "    cookie_click = driver.find_element(By.XPATH,'//*[@id=\"onetrust-accept-btn-handler\"]')\n",
    "    cookie_click.click()\n",
    "    time.sleep(1)\n",
    "    # Enter city name into search bar and navigate to restaurants tab    \n",
    "    search_bar = driver.find_element(By.XPATH,'//*[@id=\"lithium-root\"]/main/div[3]/div/div/div/form/input[1]')\n",
    "    search_bar.click()\n",
    "    search_bar.send_keys(city_name)\n",
    "    search_bar.send_keys(Keys.ENTER)\n",
    "    time.sleep(2)\n",
    "    rest_tab = driver.find_element(By.XPATH, '//*[@id=\"search-filters\"]/ul/li[4]/a')\n",
    "    rest_tab.click()\n",
    "    time.sleep(2)\n",
    "    # Click on the specified number of restaurants    \n",
    "    for i in range(1, num_restaurants + 1):\n",
    "        xpath = '//*[@id=\"BODY_BLOCK_JQUERY_REFLOW\"]/div[2]/div/div[2]/div/div/div/div/div[1]/div/div[1]/div/div[3]/div/div[1]/div/div[2]/div/div/div[{}]'.format(i)\n",
    "        rest = driver.find_element(By.XPATH, xpath)\n",
    "        rest.click()\n",
    "    # Switch to the second tab    \n",
    "    window_handles = driver.window_handles    \n",
    "    driver.switch_to.window(window_handles[0])\n",
    "    driver.close()\n",
    "    time.sleep(1)\n",
    "    window_handles = driver.window_handles   \n",
    "    time.sleep(4)\n",
    "    # Create an empty dataframe to hold the restaurant data    \n",
    "    restaurants = pd.DataFrame(columns = ['name', 'rating', 'Num_of_reviews', 'url', 'address', 'phone', 'website'])\n",
    "    # Loop through open restaurant tabs and scrape data   \n",
    "    window_handles = driver.window_handles    \n",
    "    for i in range(num_restaurants):\n",
    "        driver.switch_to.window(window_handles[i])\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        name = soup.find('h1', {'class': 'HjBfq'}).text        \n",
    "        rating = soup.find('span', {'class': 'ZDEqb'}).text        \n",
    "        num_revs = soup.find('a', {'class': 'IcelI'}).text        \n",
    "        url = driver.current_url        \n",
    "        address = soup.find('span', {'class': 'yEWoV'}).text        \n",
    "        phone = soup.find('span', {'class': 'AYHFM'}).find('a').text        \n",
    "        website = soup.find('a', {'class': 'YnKZo Ci Wc _S C AYHFM'}).get('href')\n",
    "        restaurants = restaurants.append({'name': name, 'rating': rating, 'Num_of_reviews': num_revs, 'url': url, 'address': address, 'phone': phone, 'website': website}, ignore_index=True)\n",
    "    # Close the web driver and return the restaurant data    \n",
    "    driver.quit()\n",
    "    return restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>Num_of_reviews</th>\n",
       "      <th>url</th>\n",
       "      <th>address</th>\n",
       "      <th>phone</th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Restauracja Zapiecek</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3,004 reviews</td>\n",
       "      <td>https://www.tripadvisor.com/Restaurant_Review-...</td>\n",
       "      <td>Swietojanska 13, Warsaw 00-266 Poland</td>\n",
       "      <td>+48 22 635 61 09</td>\n",
       "      <td>http://www.zapiecek.eu/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stolica</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1,656 reviews</td>\n",
       "      <td>https://www.tripadvisor.com/Restaurant_Review-...</td>\n",
       "      <td>Szeroki Dunaj 1/3 Stare Miasto, Warsaw 00-255 ...</td>\n",
       "      <td>+48 604 598 764</td>\n",
       "      <td>https://restauracjastolica.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beef n' Pepper</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2,834 reviews</td>\n",
       "      <td>https://www.tripadvisor.com/Restaurant_Review-...</td>\n",
       "      <td>Nowogrodzka 47a, Warsaw 00-695 Poland</td>\n",
       "      <td>+48 785 025 025</td>\n",
       "      <td>http://www.beefandpepper.pl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name rating Num_of_reviews  \\\n",
       "0  Restauracja Zapiecek   4.0   3,004 reviews   \n",
       "1               Stolica   4.5   1,656 reviews   \n",
       "2        Beef n' Pepper   5.0   2,834 reviews   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.tripadvisor.com/Restaurant_Review-...   \n",
       "1  https://www.tripadvisor.com/Restaurant_Review-...   \n",
       "2  https://www.tripadvisor.com/Restaurant_Review-...   \n",
       "\n",
       "                                             address             phone  \\\n",
       "0              Swietojanska 13, Warsaw 00-266 Poland  +48 22 635 61 09   \n",
       "1  Szeroki Dunaj 1/3 Stare Miasto, Warsaw 00-255 ...   +48 604 598 764   \n",
       "2              Nowogrodzka 47a, Warsaw 00-695 Poland   +48 785 025 025   \n",
       "\n",
       "                          website  \n",
       "0         http://www.zapiecek.eu/  \n",
       "1  https://restauracjastolica.com  \n",
       "2     http://www.beefandpepper.pl  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_tripadvisor_restaurants('Warsaw', 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "---\n",
    "### Web Scraping\n",
    "- [Web Scraping with Python](https://www.scrapethissite.com/pages/)\n",
    "\n",
    "### Beautiful Soup\n",
    "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Beautiful Soup Tutorial](https://www.dataquest.io/blog/web-scraping-tutorial-python/)\n",
    "- [Beautiful Soup Tutorial Youtube](https://www.youtube.com/watch?v=ng2o98k983k)\n",
    "\n",
    "### Selenium\n",
    "- [Selenium Documentation](https://selenium-python.readthedocs.io/)\n",
    "- [Complex Selenium Tutorial in Java](https://www.guru99.com/selenium-tutorial.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f5583cf1d9466b5c27e75c89cc6b383bed5736d6b16c51c8074d8690011a952"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

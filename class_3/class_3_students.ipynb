{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 3\n",
    "\n",
    "## Computer Vision\n",
    "---\n",
    "\n",
    "### Introduction to Computer Vision\n",
    "\n",
    "Computer vision is a subfield of artificial intelligence and computer science that focuses on enabling machines to interpret and understand visual data from the world around them. The goal of computer vision is to enable computers to analyze, process, and understand images or videos in a way that mimics human vision.\n",
    "\n",
    "### Applications of computer vision are numerous, including:\n",
    "\n",
    "- Autonomous vehicles: Computer vision is essential in enabling autonomous vehicles to perceive their environment and make decisions based on what they see.\n",
    "\n",
    "- Healthcare: Computer vision can be used in medical imaging to identify and diagnose diseases from X-rays, MRIs, and other medical images.\n",
    "\n",
    "- Surveillance: Computer vision can be used in security systems to detect and track people and objects in real-time.\n",
    "\n",
    "- Robotics: Computer vision is crucial in enabling robots to perceive their surroundings, identify objects, and interact with their environment.\n",
    "\n",
    "### Some of the challenges in computer vision include:\n",
    "\n",
    "1. Image recognition: Teaching a computer to recognize objects and patterns within an image is a complex process, especially with variations in lighting, orientation, and image quality.\n",
    "\n",
    "2. Object tracking: Following a moving object across multiple frames in a video can be challenging, especially when objects are occluded, obscured, or change in appearance.\n",
    "\n",
    "3. Depth perception: Determining the distance of objects from the camera is difficult, and it requires sophisticated algorithms and sensors to accurately measure depth.\n",
    "\n",
    "4. Understanding context: Computers lack the contextual knowledge and common sense that humans possess, making it difficult for them to understand the meaning and significance of visual information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer vision libraries\n",
    "- [OpenCV](https://opencv.org/)\n",
    "  - OpenCV (Open Source Computer Vision): A powerful and widely used library for image and video processing that supports a wide range of algorithms and features for object detection, recognition, and tracking.\n",
    "\n",
    "- [TensorFlow](https://www.tensorflow.org/)\n",
    "  - TensorFlow: An open-source machine learning library for building and training deep neural networks that can be used for various computer vision tasks such as image classification, object detection, and segmentation.\n",
    "\n",
    "- [Scikit-learn](https://scikit-learn.org/stable/)\n",
    "  - scikit-image: A collection of algorithms and tools for image processing and computer vision that provide a simple and intuitive interface for performing common tasks such as segmentation, feature extraction, and filtering.\n",
    "  \n",
    "- [Keras](https://keras.io/)\n",
    "  - Keras: A high-level neural network API that can be used with TensorFlow, Theano, or CNTK backend to build and train neural networks for computer vision tasks.\n",
    "\n",
    "- [PyTorch](https://pytorch.org/)\n",
    "  - PyTorch: A popular deep learning framework that provides a flexible and efficient way to build and train neural networks for computer vision applications.\n",
    "\n",
    "\n",
    "- [Dlib](http://dlib.net/)\n",
    "  - DLIB: A library of machine learning algorithms and tools for solving various computer vision problems such as face detection, landmark detection, and object tracking."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of OpenCV\n",
    "\n",
    "### Reading and displaying images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load an image from file\n",
    "img = cv2.imread('images/cat.jpg')\n",
    "\n",
    "cv2.startWindowThread()\n",
    "# Display the image\n",
    "cv2.imshow('Cat', img)\n",
    "# Wait for input\n",
    "cv2.waitKey(0)\n",
    "# close the window\n",
    "cv2.destroyAllWindows()\n",
    "# Mac Users hack (sic!) https://stackoverflow.com/questions/6116564/destroywindow-does-not-close-window-on-mac-using-python-and-opencv\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load an image from file\n",
    "img = cv2.imread('images/tree.jpg')\n",
    "\n",
    "cv2.startWindowThread()\n",
    "\n",
    "cv2.imshow('Tree', img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyWindow('Tree')\n",
    "\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `imread()` function returns a NumPy array representing the image, which can be further processed using other OpenCV functions or standard NumPy operations. The `imshow()` function displays the image in a window, and the `waitKey()` function waits for a key press to close the window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a video\n",
    "\n",
    "import cv2\n",
    "\n",
    "capture = cv2.VideoCapture('videos/ski.mp4')\n",
    "\n",
    "while(True):\n",
    "    ret, frame = capture.read()\n",
    "    cv2.imshow('Ski', frame)\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "      break\n",
    "  \n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the `VideoCapture()` function is used to open a video file named `'ski.mp4'`  in the `videos` directory. Then, a loop is used to read the video frames one by one using the `read()` function of the video capture object. The loop will continue until there are no more frames to read or the user presses the `'q'` key to exit."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing and rescaling images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: grid; grid-template-columns: 2fr 1fr; gap: 16px\">\n",
    "<div>\n",
    "<p>\n",
    "Rescaling and resizing images are two common operations used in image processing and computer vision applications to adjust the size and aspect ratio of an image.\n",
    "\n",
    "Rescaling an image involves changing the size of the image while maintaining the same aspect ratio. This operation can be performed by multiplying the width and height of the original image by a scaling factor, which can be greater than or less than one depending on whether you want to increase or decrease the size of the image. Rescaling an image can be useful for reducing the computational load of processing large images or increasing the resolution of small images.\n",
    "\n",
    "Resizing an image, on the other hand, involves changing the size and aspect ratio of the image by specifying new width and height dimensions. This operation can be performed by interpolating the pixel values of the original image to fit the new dimensions, which can result in loss of information or distortion if the aspect ratio is significantly changed. Resizing an image can be useful for preparing images for specific display or processing requirements, such as training a neural network or displaying images in a specific aspect ratio.\n",
    "\n",
    "In summary, rescaling and resizing images are important operations in image processing and computer vision that can be used to adjust the size and aspect ratio of an image for specific applications. It is important to understand the differences between these two operations and choose the appropriate method depending on your specific needs.\n",
    "</p>\n",
    "</div>\n",
    "<img src=\"https://cdn.mobilesyrup.com/wp-content/uploads/2019/02/zoom-enhance.gif\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_frame(frame, scale=0.50):\n",
    "    width = int(frame.shape[1] * scale)\n",
    "    height = int(frame.shape[0] * scale)\n",
    "    dim = (width, height)\n",
    "    return cv2.resize(frame, dim, interpolation = cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture('videos/ski.mp4')\n",
    "\n",
    "while(True):\n",
    "    ret, frame = capture.read()\n",
    "    frame_resized = rescale_frame(frame, scale=0.25)\n",
    "    cv2.imshow('Ski', frame)\n",
    "    cv2.imshow('Ski Resized', frame_resized)\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "      break\n",
    "  \n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/cat.jpg')\n",
    "cv2.imshow('Cat', img)\n",
    "cv2.imshow('Cat resized', rescale_frame(img, scale=0.25))\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyWindow('Cat')\n",
    "cv2.destroyWindow('Cat resized')\n",
    "\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing and writing on images and videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing a red image\n",
    "blank = np.zeros((300,300,3), dtype='uint8')\n",
    "\n",
    "blank[:] = 0, 0, 255\n",
    "\n",
    "cv2.imshow('Blank', blank)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing a partialy red image\n",
    "blank = np.zeros((300,300,3), dtype='uint8')\n",
    "\n",
    "blank[100:150, 100:150] = 0, 0, 255\n",
    "\n",
    "cv2.imshow('Blank', blank)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing a partialy red image with a rectangle\n",
    "blank = np.zeros((300,300,3), dtype='uint8')\n",
    "\n",
    "# Background\n",
    "blank[100:150, 100:150] = 0, 0, 255\n",
    "\n",
    "cv2.rectangle(blank, (200,200), (300, 300), (255,255,0), thickness=cv2.FILLED)\n",
    "cv2.rectangle(blank, (0,0), (blank.shape[1]//4, blank.shape[0]//4), (0,255,0), thickness=cv2.FILLED)\n",
    "cv2.circle(blank, (blank.shape[1]//2, blank.shape[0]//2), 40, (255,255,255), thickness=2)\n",
    "\n",
    "cv2.imshow('Blank', blank)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">**Task**</span>\n",
    "\n",
    "Recreate the following image using OpenCV:\n",
    "\n",
    "<img src=\"images/face1.png\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code goes here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Functions\n",
    "\n",
    "#### Converting to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/cat.jpg')\n",
    "cv2.imshow('Cat', img)\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "cv2.imshow('Gray', gray)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blurring images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/cat.jpg')\n",
    "cv2.imshow('Cat', img)\n",
    "\n",
    "blur = cv2.GaussianBlur(img, (13,13), cv2.BORDER_DEFAULT)\n",
    "\n",
    "cv2.imshow('Blur', blur)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/cat.jpg')\n",
    "cv2.imshow('Cat', img)\n",
    "\n",
    "\n",
    "canny = cv2.Canny(img, 125, 175)\n",
    "blur = cv2.GaussianBlur(img, (9,9), cv2.BORDER_DEFAULT)\n",
    "canny_blur = cv2.Canny(blur, 125, 175)\n",
    "\n",
    "cv2.imshow('Canny', canny)\n",
    "cv2.imshow('Canny blur', canny_blur)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dilating and eroding images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/cat.jpg')\n",
    "cv2.imshow('Cat', img)\n",
    "\n",
    "\n",
    "blur = cv2.GaussianBlur(img, (9,9), cv2.BORDER_DEFAULT)\n",
    "canny_blur = cv2.Canny(blur, 125, 175)\n",
    "dilated = cv2.dilate(canny_blur, (3,3), iterations=3)\n",
    "\n",
    "cv2.imshow('Canny', canny_blur)\n",
    "cv2.imshow('Dilated', dilated)\n",
    "\n",
    "eroded = cv2.erode(dilated, (3,3), iterations=3)\n",
    "\n",
    "cv2.imshow('Eroded', eroded)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resizing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/cat.jpg')\n",
    "cv2.imshow('Cat', img)\n",
    "\n",
    "resized = cv2.resize(img, (500,500), interpolation=cv2.INTER_AREA)\n",
    "cv2.imshow('Resized', resized)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cropping images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/cat.jpg')\n",
    "cv2.imshow('Cat', img)\n",
    "\n",
    "cropped = img[600:900, 900:1600]\n",
    "cv2.imshow('Cropped', cropped)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">**Task**</span>\n",
    "\n",
    "Crop the `tree` image to show only the tree. Then, resize the cropped image to 250 x 250 pixels. \n",
    "\n",
    "Prepare the image for display by converting it to grayscale and blurring it.\n",
    "\n",
    "Do the edge detection on the blurred image, and then dilate the edges to make them thicker.\n",
    "\n",
    "Finally, save the detected edges image to the `images` directory.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transitions\n",
    "\n",
    "#### Translating images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/husky.jpg')\n",
    "cv2.imshow('Original', img)\n",
    "\n",
    "x_offset = 100\n",
    "y_offset = 100\n",
    "\n",
    "transMat = np. float32([[1,0 ,x_offset], [0,1,y_offset]])\n",
    "dimensions = (img.shape[1], img.shape[0])\n",
    "translated = cv2.warpAffine(img, transMat, dimensions)\n",
    "\n",
    "\n",
    "cv2.imshow('Translated', translated)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotating images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/husky.jpg')\n",
    "cv2.imshow('Original', img)\n",
    "\n",
    "angle = 45\n",
    "\n",
    "rotateMat = cv2.getRotationMatrix2D((img.shape[1]//2, img.shape[0]//2), angle, 1)\n",
    "dimensions = (img.shape[1], img.shape[0])\n",
    "rotated = cv2.warpAffine(img, rotateMat, dimensions)\n",
    "\n",
    "rotated2 = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "cv2.imshow('Rotated', rotated)\n",
    "cv2.imshow('Rotated 2', rotated2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flipping images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/husky.jpg')\n",
    "cv2.imshow('Original', img)\n",
    "\n",
    "flip = cv2.flip(img, 1)\n",
    "cv2.imshow('Flip', flip)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture('videos/ski.mp4')\n",
    "\n",
    "while(True):\n",
    "    ret, frame = capture.read()\n",
    "    frame_resized = rescale_frame(frame, scale=0.25)\n",
    "    cv2.imshow('Ski', frame)\n",
    "    cv2.imshow('Ski Resized', frame_resized)\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "      break\n",
    "  \n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contours\n",
    "\n",
    "Contours are a useful tool in image recognition because they provide a way to represent the shape and boundaries of objects in an image. A contour is a curve that connects continuous points of an object with the same intensity or color. By finding and analyzing contours in an image, we can extract important information about the objects present in the image, such as their shape, size, orientation, and position relative to other objects.\n",
    "\n",
    "Contour detection is a common preprocessing step in many computer vision applications, such as object detection, segmentation, and tracking. For example, in object detection, we can use contours to isolate and identify individual objects in an image, by applying techniques such as thresholding and morphological operations to extract the contours from the image. In image segmentation, we can use contours to partition an image into regions based on their shapes and boundaries, allowing us to extract meaningful features from the image.\n",
    "\n",
    "Overall, contours provide a powerful and flexible way to represent the structure and content of images, and are a fundamental tool in many image recognition and computer vision tasks.\n",
    "\n",
    "#### Finding contours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/husky.jpg')\n",
    "cv2.imshow('Original', img)\n",
    "\n",
    "angle = 45\n",
    "\n",
    "rotateMat = cv2.getRotationMatrix2D((img.shape[1]//2, img.shape[0]//2), angle, 1)\n",
    "dimensions = (img.shape[1], img.shape[0])\n",
    "rotated = cv2.warpAffine(img, rotateMat, dimensions)\n",
    "\n",
    "rotated2 = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "cv2.imshow('Rotated', rotated)\n",
    "cv2.imshow('Rotated 2', rotated2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/tree.jpg')\n",
    "cv2.imshow('Dog', img)\n",
    "\n",
    "# Scenario 1\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "canny = cv2.Canny(gray, 125, 175)\n",
    "\n",
    "# Scenario 2 (with blur)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "blur = cv2.GaussianBlur(gray, (5,5), cv2.BORDER_DEFAULT)\n",
    "canny = cv2.Canny(blur, 125, 175)\n",
    "\n",
    "cv2.imshow('Gray', gray)\n",
    "cv2.imshow('Canny', canny)\n",
    "\n",
    "contours, hierarchies = cv2.findContours(canny, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "print(f'{len(contours)} contour(s) found.')\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/cat.jpg')\n",
    "cv2.imshow('Original', img)\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('Gray', gray)\n",
    "hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "cv2.imshow('hsv', hsv)\n",
    "lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "cv2.imshow('lab', lab)\n",
    "rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "cv2.imshow('rgb', rgb)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/parrot.jpg')\n",
    "cv2.imshow('Original', img)\n",
    "\n",
    "b,g,r = cv2.split(img)\n",
    "cv2.imshow('Blue', b)\n",
    "cv2.imshow('Green', g)\n",
    "cv2.imshow('Red', r)\n",
    "\n",
    "merged = cv2.merge([b,g,r])\n",
    "\n",
    "cv2.imshow('Merged', merged)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bluring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/parrot.jpg')\n",
    "cv2.imshow('Original', img)\n",
    "\n",
    "average33 = cv2.blur(img, (3,3))\n",
    "cv2.imshow('Average 3 x 3', average33)\n",
    "\n",
    "average77 = cv2.blur(img, (7,7))\n",
    "cv2.imshow('Average 7 x 7', average77)\n",
    "\n",
    "gaussian33 = cv2.GaussianBlur(img, (3,3), 0)\n",
    "cv2.imshow('Gaussian 3 x 3', gaussian33)\n",
    "\n",
    "gaussian77 = cv2.GaussianBlur(img, (7,7), 0)\n",
    "cv2.imshow('Gaussian 7 x 7', gaussian77)\n",
    "\n",
    "median33 = cv2.medianBlur(img, 3)\n",
    "cv2.imshow('Median 3 x 3', median33)\n",
    "\n",
    "median77 = cv2.medianBlur(img, 7)\n",
    "cv2.imshow('Median 7 x 7', median77)\n",
    "\n",
    "bilateral = cv2.bilateralFilter(img, 10, 40, 30)\n",
    "cv2.imshow('Bilateral', bilateral)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() \n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bitwise operations & masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/cat.jpg')\n",
    "# cv2.imshow('Original', img)\n",
    "\n",
    "blank = np.zeros(img.shape[:2], dtype='uint8')\n",
    "\n",
    "circle = cv2.circle(blank.copy(), (img.shape[1]//2, img.shape[0]//2), 350, 255, -1)\n",
    "# cv2.imshow('Circle', circle)\n",
    "\n",
    "bitwise_and = cv2.bitwise_and(img, img, mask=circle)\n",
    "cv2.imshow('Bitwise AND', bitwise_and)\n",
    "\n",
    "bitwise_not = cv2.bitwise_not(img, img, mask=circle)\n",
    "cv2.imshow('Bitwise NOT', bitwise_not)\n",
    " \n",
    "# cv2.imshow('Circle', circle) \n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an image of a cat. Do a bitwise operations to mask out the cat's yellow eyes. Use color channels . Add greyscale to everything else.\n",
    "\n",
    "img = cv2.imread('images/cat.jpg')\n",
    "\n",
    "blank = np.zeros(img.shape[:2], dtype='uint8')\n",
    "circle = cv2.circle(blank.copy(), (img.shape[1]//2 + 180, img.shape[0]//2 + 50), 275, 255, -1)\n",
    "\n",
    "hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "cv2.imshow('hsv', hsv) \n",
    "\n",
    "\n",
    "bitwise_and = cv2.bitwise_and(img, img, mask=circle)\n",
    "cv2.imshow('Bitwise AND', bitwise_and) \n",
    "# Detect yellow collor in the bitwise_and image\n",
    "mask = cv2.inRange(bitwise_and, (80, 100, 0), (255, 255, 255))\n",
    "cv2.imshow('Mask', mask) \n",
    "\n",
    "# Bitwise AND the mask and the original image to extract the yellow eyes\n",
    "eyes = cv2.bitwise_and(img, img, mask=mask)\n",
    "\n",
    "# Create a grayscale version of the image\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "mask_inv = cv2.bitwise_not(mask)\n",
    "gray_masked = cv2.bitwise_and(gray, gray, mask=mask_inv)\n",
    "\n",
    "# Invert the mask and apply it to the grayscale image to turn everything else into grayscale\n",
    "\n",
    "# Combine the extracted yellow eyes and the grayscale image\n",
    "result = cv2.add(cv2.cvtColor(gray,cv2.COLOR_GRAY2RGB), eyes)\n",
    "\n",
    "# Display the original image and the result\n",
    "# cv2.imshow('Original Image', img)\n",
    "# cv2.imshow('Mask', circle)\n",
    "cv2.imshow('Masked Image', result)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/husky.jpg')\n",
    "cv2.imshow('Original', img)\n",
    "\n",
    "angle = 45\n",
    "\n",
    "rotateMat = cv2.getRotationMatrix2D((img.shape[1]//2, img.shape[0]//2), angle, 1)\n",
    "dimensions = (img.shape[1], img.shape[0])\n",
    "rotated = cv2.warpAffine(img, rotateMat, dimensions)\n",
    "\n",
    "rotated2 = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "cv2.imshow('Rotated', rotated)\n",
    "cv2.imshow('Rotated 2', rotated2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">**Task**</span>\n",
    "\n",
    "Recreate the following image using OpenCV:\n",
    "\n",
    "<img src=\"images/cat_eyes.jpg\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/cat.jpg')\n",
    "\n",
    "# Might be useful to start with:\n",
    "# Convert the image to HSV space\n",
    "hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "# Define lower and upper bounds of yellow color in HSV space\n",
    "lower_yellow = np.array([10, 100, 100])\n",
    "upper_yellow = np.array([30, 255, 255])\n",
    "# Create a mask of yellow pixels\n",
    "yellow_mask = cv2.inRange(hsv, lower_yellow, upper_yellow)\n",
    "\n",
    "\n",
    "#\n",
    "# Code goes here\n",
    "#\n",
    "\n",
    "\n",
    "cv2.imshow('Original Image', img)\n",
    "cv2.imshow('Yellow Eyes', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thresholding\n",
    "\n",
    "Thresholding is a technique used in image processing and computer vision to separate objects or regions from the background based on their intensity values. The basic idea of thresholding is to convert an image into a binary image, where the pixels are classified as either foreground or background based on a specified threshold value. We use thresholding for several purposes, such as image segmentation, object detection, feature extraction, image enhancement, and image analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/cat.jpg')\n",
    "cv2.imshow('Dog', img)\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "ret, thresh = cv2.threshold(gray, 75, 255, cv2.THRESH_BINARY)\n",
    "cv2.imshow('Thresh', thresh)\n",
    "\n",
    "ret, thresh_inv = cv2.threshold(gray, 75, 255, cv2.THRESH_BINARY_INV)\n",
    "cv2.imshow('Thresh Inv', thresh_inv)\n",
    "\n",
    "adaptive_thresh_m = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 13, 5)\n",
    "cv2.imshow('Adaptive Thresh Mean', adaptive_thresh_m)\n",
    "\n",
    "adaptive_thresh_g = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 13, 5)\n",
    "cv2.imshow('Adaptive Thresh Gaussian', adaptive_thresh_g)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edge detection - Laplacian and Sobel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/cat.jpg')\n",
    "cv2.imshow('Original', img)\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('Gray', gray)\n",
    "\n",
    "# Laplacian\n",
    "lap = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "lap = np.uint8(np.absolute(lap))\n",
    "cv2.imshow('Laplacian', lap)\n",
    "\n",
    "# Sobel\n",
    "sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0)\n",
    "sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1)\n",
    "combined_sobel = cv2.bitwise_or(sobelx, sobely)\n",
    " \n",
    "cv2.imshow('Sobel X', sobelx)\n",
    "cv2.imshow('Sobel Y', sobely)\n",
    "cv2.imshow('Combined Sobel', combined_sobel)\n",
    " \n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge detection is a fundamental process in computer vision that aims to identify boundaries in an image. These boundaries represent regions where the pixel intensities undergo significant changes, such as corners, lines, or edges. The edges can be used to identify objects, track movements, and perform image segmentation.\n",
    "\n",
    "There are several methods for edge detection, and the most popular ones are:\n",
    "\n",
    "**Sobel Operator**: The Sobel operator is a gradient-based approach that detects edges by computing the gradient of the image intensity function.\n",
    "\n",
    "**Canny Edge Detector**: The Canny edge detector is a multi-stage algorithm that involves smoothing the image, calculating the gradient, applying non-maximum suppression, and hysteresis thresholding.\n",
    "\n",
    "**Laplacian Operator**: The Laplacian operator is a second-order differential operator that detects edges by computing the Laplacian of the image intensity function.\n",
    "\n",
    "\n",
    "The differences between these methods lie in their accuracy, speed, and robustness to noise. For example, the Sobel operator is fast and computationally efficient but may miss small details or produce noisy edges. The Canny edge detector is more accurate and robust but requires more computational resources. The Laplacian operator is sensitive to noise but can detect edges at different scales. Choosing the right method depends on the application requirements and the characteristics of the image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Haar Cascades classifiers](https://github.com/opencv/opencv/tree/master/data/haarcascades)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread('images/woman.jpg')\n",
    "cv2.imshow('Original', img)\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('Gray', gray)\n",
    "\n",
    "haar_cascage_face = cv2.CascadeClassifier('haar_face.xml')\n",
    "faces_rect = haar_cascage_face.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5)\n",
    "\n",
    "print(f'Number of faces found = {len(faces_rect)}')\n",
    "\n",
    "for (x, y, w, h) in faces_rect:\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 0, 255), thickness=4)\n",
    "\n",
    "cv2.imshow('Detected Faces', img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group of people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread('images/family.jpg')\n",
    "cv2.imshow('Original', img)\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('Gray', gray)\n",
    "\n",
    "haar_cascage_face = cv2.CascadeClassifier('haar_face.xml')\n",
    "faces_rect = haar_cascage_face.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=3)\n",
    "\n",
    "print(f'Number of faces found = {len(faces_rect)}')\n",
    "\n",
    "for (x, y, w, h) in faces_rect:\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 0, 255), thickness=4)\n",
    "\n",
    "cv2.imshow('Detected Faces', img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img = cv2.imread('images/employees.jpg')\n",
    "cv2.imshow('Original', img)\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('Gray', gray)\n",
    "\n",
    "haar_cascage_face = cv2.CascadeClassifier('haar_face.xml')\n",
    "faces_rect = haar_cascage_face.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "print(f'Number of faces found = {len(faces_rect)}')\n",
    "\n",
    "for (x, y, w, h) in faces_rect:\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 0, 255), thickness=4)\n",
    "\n",
    "cv2.imshow('Detected Faces', img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face recognition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Face recognition model training using LBPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "DIR = r'images/recognizer'\n",
    "people = [i for i in os.listdir(DIR) if not i.startswith('.')]\n",
    "\n",
    "haar_cascade_face = cv2.CascadeClassifier('haar_face.xml')\n",
    "\n",
    "def create_train():\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for person in people:\n",
    "        path = os.path.join(DIR, person)\n",
    "        label = people.index(person)\n",
    "        \n",
    "        for img in os.listdir(path):\n",
    "            if (img.startswith('.')):\n",
    "                continue\n",
    "            img_path = os.path.join(path, img)\n",
    "            img = cv2.imread(img_path)\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            faces_rect = haar_cascade_face.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "            \n",
    "            for (x, y, w, h) in faces_rect:\n",
    "                faces_roi = gray[y:y+h, x:x+w]\n",
    "                features.append(faces_roi)\n",
    "                labels.append(label)\n",
    "            \n",
    "    return (features, labels)\n",
    "\n",
    "\n",
    "features, labels = create_train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "face_recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "features = np.array(features, dtype='object')\n",
    "labels = np.array(labels)\n",
    "\n",
    "face_recognizer.train(features, labels)\n",
    "\n",
    "face_recognizer.save('train/face_trained.yml')\n",
    "\n",
    "np.save('train/features.npy', features)\n",
    "np.save('train/labels.npy', labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing out the face recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = r'images/recognizer'\n",
    "people = [i for i in os.listdir(DIR) if not i.startswith('.')]\n",
    "img = cv2.imread('images/test/biden.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('Test', gray)\n",
    "\n",
    "features = np.load('train/features.npy', allow_pickle=True)\n",
    "labels = np.load('train/labels.npy')\n",
    "face_recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "face_recognizer.read('train/face_trained.yml')\n",
    "\n",
    "haar_cascade_face = cv2.CascadeClassifier('haar_face.xml')\n",
    "\n",
    "faces_rect = haar_cascade_face.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\n",
    "\n",
    "for (x, y, w, h) in faces_rect:\n",
    "    faces_roi = gray[y:y+h, x:x+w]\n",
    "    label, confidence = face_recognizer.predict(faces_roi)\n",
    "    print(f'Label = {people[label]} with a confidence of {confidence}')\n",
    "    cv2.putText(img, f'{str(people[label])} - {round(confidence,2)}%', (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1.0, (0, 255, 0), thickness=2)\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 0, 255), thickness=2)\n",
    "\n",
    "\n",
    "cv2.imshow('Detected Face', img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "haar_cascade_face = cv2.CascadeClassifier('haar_face.xml')\n",
    "\n",
    "\n",
    "while True:\n",
    "    check, frame = cam.read()\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imshow('Test', gray)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "haar_cascade_face = cv2.CascadeClassifier('haar_smile.xml')\n",
    "\n",
    "while True:\n",
    "    check, frame = cam.read()\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imshow('gray', gray)\n",
    "    \n",
    "    faces_rect = haar_cascade_face.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10)\n",
    "    for (x, y, w, h) in faces_rect:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), thickness=2)\n",
    "    \n",
    "    cv2.imshow('Detected Face', frame)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "<a href=\"https://pl.freepik.com/darmowe-zdjecie/dosc-piekna-kobieta-z-blond-dlugie-wlosy-o-podekscytowany-i-szczesliwy-wyraz-twarzy_9116613.htm#page=2&query=usmiech&position=2&from_view=search&track=sph\">Obraz autorstwa cookie_studio</a> na Freepik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
